# Задача 4 — Витрины и аналитика (ClickHouse)

## Что делает моя часть

Данный этап отвечает за аналитический слой платформы и включает:

- построение агрегированных витрин в ClickHouse;
- перенос данных из Core DWH (PostgreSQL) в витрины с помощью Spark;
- подготовку SQL-запросов для аналитики и сравнения производительности.

Витрины предназначены для быстрого выполнения тяжёлых аналитических запросов (BI / reporting), которые неэффективно выполнять напрямую на DWH.

---

## Архитектурное место этапа

Общий пайплайн данных:

OLTP / генераторы данных  
→ Kafka / MinIO  
→ Spark ETL  
→ Core DWH (PostgreSQL)  
→ Spark Aggregations  
→ ClickHouse (аналитические витрины)

Моя часть начинается **после заполнения Core DWH** и не зависит от стримингового контура.

---

## Какие витрины реализованы

### 1. Витрина по маршрутам и часам (`mart_route_hour`)

Гранулярность:  
**маршрут × дата × час**

Метрики:
- количество поездок;
- средняя стоимость поездки;
- средняя длительность;
- средняя скорость;
- средняя задержка.

---

### 2. Витрина по городам и дням (`mart_city_day`)

Гранулярность:  
**город × дата**

Метрики:
- общее количество поездок;
- суммарная выручка;
- средний чек;
- количество уникальных пользователей.

---

## DDL ClickHouse

DDL витрин находится в файле:

```
clickhouse/dclickhouse_marts_ddl.sql
```

Особенности:
- `MergeTree`;
- партиционирование по дате;
- `ORDER BY` по измерениям;
- типы данных оптимизированы под аналитику.

---

## Spark job для построения витрин

Файл:
```
spark/spark_build_marts.py
```

Что делает job:
1. читает факты и измерения из Core DWH;
2. выполняет join и агрегации;
3. формирует витрины;
4. записывает результат в ClickHouse через JDBC.

---

## Предусловия

Перед запуском Spark job:

- инфраструктура поднята (`docker compose up -d`);
- Core DWH в Postgres заполнен;
- DDL витрин применён в ClickHouse.

---

## Переменные окружения

### PostgreSQL (Core DWH)

- PG_HOST
- PG_PORT
- PG_DB
- PG_USER
- PG_PASSWORD

### ClickHouse

- CH_HOST
- CH_PORT
- CH_DB
- CH_USER
- CH_PASSWORD

---

## Запуск Spark job

Пример запуска внутри docker-сети:

```
spark-submit \
  --master spark://spark-master:7077 \
  --packages org.postgresql:postgresql:42.7.3,com.clickhouse:clickhouse-jdbc:0.6.5:all \
  spark/jobs/spark_build_marts.py \
  --mode overwrite
```

---

## SQL для аналитики и сравнения производительности

SQL-запросы:

```
benchmarks/benchmark_queries.sql
```

Содержат пары запросов:
- к Postgres (Core DWH);
- к ClickHouse (витрины).

---

## Проверки после загрузки

```sql
SELECT count() FROM mart_route_hour;
SELECT count() FROM mart_city_day;
```

---

## Идемпотентность

В учебной реализации используется:
- полная пересборка витрин (`overwrite`);
- отсутствие дубликатов при повторных запусках.

---

## Результат этапа

- данные агрегированы в ClickHouse;
- аналитические запросы выполняются значительно быстрее;
- платформа готова к BI-аналитике.
